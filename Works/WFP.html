<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Works and Contributions at PICO</title>
    <link rel="stylesheet" href="../project-styles.css">
    <link rel="stylesheet" href="../styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <link rel="icon" href="../Resources/Rongxuan.jpeg" type="image/x-icon">

</head>

<body>
    <header>
        <div class="image-container">
            <img src="../Resources/PICO.png" alt="PICO" style="width:100%;">
            <div class="overlay-text">Works at PICO</div>
        </div>
    </header>
    <!-- Navigation Bar -->
    <nav class="top-nav">
        <ul>
            <li><a href="#overview">Overview</a></li>
            <li><a href="#xri-framework">XR Interaction Framework</a></li>
            <li><a href="#os-cases">OS Cases</a></li>
            <li><a href="#XR+AI-HCI-research">XR+AI HCI Research</a></li>
            <li><a href="#gameplay">Gameplay</a></li>
            <li><a href="../index.html">← Home </a></li>


        </ul>
    </nav>


    <main>
        <!-- Home / Overview Section -->
        <section id="overview">
            <h2>Overview</h2>

            <p>
                🏢As a leading XR company, <strong>PICO</strong> specializes in <strong>XR hardware and immersive
                    interaction technologies</strong>, driving innovation in spatial computing and user experience.
            </p>
            <p>
                👨‍💻During my time at <strong>PICO</strong>, I contributed to the <strong>definition and innovation of XR
                    interaction frameworks</strong>, focusing on <strong>interaction standardization, novel input
                    methods, and immersive gameplay experience design</strong>. My work spanned <strong>technical
                    research, hardware-software exploration, and system architecture development</strong>, helping to
                enhance <strong>user interactions and interaction efficiency in XR environments</strong>.
            </p>
            <p style="color: darkblue;">
                🔒All the content presented here has either been <strong>publicly released</strong>, is
                <strong>non-confidential</strong>, or has undergone <strong>proper de-identification</strong> to
                ensure compliance with information security and confidentiality standards.
            </p>

        </section>
        <!-- HCI Framework Section -->
        <section id="xri-framework">
            <h2>XR Interaction Framework</h2>


            <p>
                The <strong>XR Interaction Framework</strong> provides a structured approach to defining interaction
                paradigms across various XR platforms.
                It integrates <span class="highlight">interaction devices, event systems, solvers, and interactor
                    logic</span> to create seamless and intuitive
                user experiences in <strong>VR, AR, and MR</strong> environments.
            </p>
            <p>
                This framework references industry-standard toolkits:
            </p>
            <ul>
                <li> <strong>XRI</strong> (XR Interaction Toolkit) – Unity's modular framework for handling
                    interactions.</li>
                <li> <strong>MRTK</strong> (Mixed Reality Toolkit) – Optimized for HoloLens and cross-platform MR
                    development.</li>
                <li> <strong>AndroidXR</strong> – Google's XR framework integrating hand-tracking, eye-tracking, and
                    controllers.</li>
                <li> <strong>VisionOS</strong> – Apple's spatial computing framework for <em>visionOS
                        applications</em>.</li>
            </ul>
            <p>⚠️ <em>The diagrams presented here are <strong>abridged representations</strong> of my formal work and
                    have been <strong>de-identified</strong> for confidentiality purposes.</em></p>

            <h3>XR Interaction Framework Components</h3>
            <img src="../Resources/XRInteractionFramework.png" alt="HCI Framework">

            <h4>Interactive Devices</h4>
            <ul>
                <li><strong>Hardware Devices:</strong> Controllers, Mouse, Touchpad, Game Controllers, Wearables (Rings,
                    Wristbands).</li>

                <li><strong>Wetware Devices:</strong> Hands ✋, Eyes 👀, Head 🧑‍🦱, Mouth 🗣️ as biological input
                    interfaces.</li>
            </ul>

            <h4>Interactors</h4>
            <ul>
                <li>👉 <strong>Direct Interactors:</strong> Pinch, Poke interactions.</li>

                <li>🎯 <strong>Indirect Interactors:</strong> Raycasting-based (Hand Ray, Controller Ray, Gaze Ray).
                </li>
                <li>🎤 <strong>Functional Interactors:</strong> Teleport, Socket, Speech Input.</li>
            </ul>

            <h4>Interaction Solvers</h4>
            <ul>
                <li>🔄 <strong>Transforming:</strong> Position, Rotation, Scale.</li>

                <li>🔍 <strong>Smoothing & Filtering:</strong> Enhancing precision.</li>
                <li>🤖 <strong>Dynamic Control:</strong> Adaptive interaction systems.</li>
            </ul>

            <h4>Interaction Events</h4>
            <ul>
                <li>🎈 <strong>Primary Basic Events:</strong> HoverEntered, SelectEntered, SelectCanceled.</li>

                <li>🎛️ <strong>Secondary Complex Events:</strong> Tap/Click, Long Press, Drag.</li>
                <li>🎡 <strong>Tertiary Events:</strong> Scale, Rotate, Gesture-based interactions.</li>
            </ul>

            <h3>Workflow of XR Interaction Process</h3>
            <br>
            <img src="../Resources/WorkFlowOfXRInteraction.png" alt="HCI Framework">


            <h4>1️⃣ User Inputs & Sensors</h4>
            <p>Interactive devices (hardware or wetware) collect user input via controllers, gestures, eye-tracking, or
                voice commands.</p>


            <h4>2️⃣ Interactor Processing</h4>
            <p>The system determines whether input is direct, indirect, or functional and triggers corresponding events
                (Raycasting, Gesture Tracking).</p>


            <h4>3️⃣ Interaction Solvers & Object Manipulation</h4>
            <p>Events are processed through solvers like <span class="highlight">Transforming, Smoothing</span>, and the
                system applies dynamic responses.</p>


            <h4>4️⃣ System Response & Feedback</h4>
            <ul>
                <li>🎨 <strong>Visual Feedback:</strong> UI responses, highlights.</li>

                <li>🎮 <strong>Haptic Feedback:</strong> Vibration, force feedback.</li>
                <li>🔊 <strong>Auditory Feedback:</strong> Sound cues, voice prompts.</li>
            </ul>

            <h4>5️⃣ Final Execution & Loopback</h4>
            <p>The system updates interaction states and ensures real-time tracking for smooth user interaction.</p>


            <h3>Conclusion</h3>
            <p>
                This XR Interaction Framework and workflow provide a standardized, modular approach for designing

                intuitive
                and efficient user interactions in XR environments.
                By referencing <span class="highlight">XRI, MRTK, AndroidXR, and VisionOS</span>, this system ensures
                compatibility across multiple XR platforms.
            </p>
            <p>⚠️ <strong>Confidentiality Notice:</strong> The content and diagrams shown here are
                <strong>de-identified</strong> representations of my work.
            </p>
        </section>

        <!-- OS Cases Section -->
        <section id="os-cases">
            <h2>OS Cases</h2>
            <h3>Optimization of OS Interactions</h3>
            <p>
                In my role at <strong>PICO</strong>, I worked on optimizing operating system interactions to improve
                user engagement and system performance. This included refining <strong>gesture recognition
                    algorithms</strong> 🤏, enhancing <strong>multi-touch capabilities</strong> 👆, and ensuring smooth
                transitions between different interaction modes. Additionally, I developed a fuzzy-gaze-like strategy for eye-tracking 👀 to enhance precision and responsiveness in user interactions.
            </p>
            <h3>Solutions for OS Apps & Services</h3>
            <p>
                I contributed to developing solutions for OS applications by implementing <strong>adaptive user
                    interfaces</strong> and integrating <strong>AI-driven interaction models</strong> 🤖. These
                solutions aimed to provide a personalized user experience, adapting to user preferences and behaviors
                across various <strong>XR platforms</strong>. Additionally, I worked on enhancing OS services such as the <strong>MR
                    safety boundary</strong> to ensure user safety, and developed <strong>spatial input methods</strong> to improve
                interaction precision and efficiency.
            </p>
        </section>
        <!-- XR HCI Research Section -->
        <section id="XR+AI-HCI-research">
            <h2>XR+AI HCI Research</h2>
            <p>
                My research in <strong>XR+AI HCI</strong> focuses on integrating <strong>artificial
                    intelligence</strong> to enhance human-computer interaction in extended reality environments. This
                involves exploring innovative ways to leverage AI for improving user experience and interaction
                efficiency 🤖.
            </p>

            <h3>AI-Driven Interaction Paradigms</h3>
            <p>
                I explored the integration of AI with XR to create more intuitive and efficient interaction paradigms.
                This research focused on developing <strong>AI-driven systems</strong> that can adapt to user
                preferences and behaviors, providing a personalized and seamless user experience 🌟.
            </p>

            <h3>Core Principles of XR UI/UX</h3>
            <p>
                The core of my research lies in understanding the fundamental principles of <strong>XR HCI</strong>,
                aiming to create intuitive and immersive user experiences. This includes studying user behavior,
                interaction patterns, and the impact of various XR technologies on user engagement 🎮.
            </p>

            <h3>Enhancing XR Interaction Performance</h3>
            <p>
                I conducted extensive research on <strong>XR interaction performance</strong> to identify bottlenecks
                and optimize system responsiveness. This research helps in enhancing the fluidity and naturalness of
                interactions, ensuring a seamless user experience ⚡.
            </p>

            <h3>Innovations in XR Interaction Devices</h3>
            <p>
                My work on <strong>XR interaction devices</strong> involves evaluating and improving the effectiveness
                of various input devices, such as controllers, hand-tracking systems, and eye-tracking technologies 👀.
                This research aims to expand the possibilities of user interaction in XR environments 🌟.
            </p>
        </section>
        <!-- Game Play Section -->
        <section id="gameplay">
            <h2>XR Gameplay</h2>

            <p>
                Before I came to the department of <strong>HCI</strong>, I worked as a <strong>technical
                    designer</strong> in the department of <strong>Tech and Art</strong>, working on some innovative
                gameplay for demonstrating the <strong>MR feature</strong> of the headset.
            </p>
            <p>
                My gameplay contributions lie in two specific projects:
            </p>

            <h3>2022 Chinese New Year Mixed-Reality Showcase</h3>
            <iframe
                src="https://player.bilibili.com/player.html?isOutside=true&aid=947638004&bvid=BV14W4y1V7zJ&cid=962281470&p=1&autoplay=0"
                width="800" height="450" scrolling="no" border="0" frameborder="no" framespacing="0"
                allowfullscreen="true"></iframe>
            <p style="text-align: center; font-style: italic; color: grey; font-size: smaller;">Bilibili video showcasing the 2022 Chinese New Year Mixed-Reality experience.</p>

            <p>
                This project demonstrates the potential of <strong>PICO 4</strong> in mixed reality (MR), featuring an
                operation activity during CNY 2022. It included a New Year gift box and several different props with MR
                features. I was in charge of the dragon dance wand that can trigger a spatial snake-like game.
            </p>
            <p>
                Watch the real-time effect recording of our outcome:
            </p>

            <!-- Videos Embed -->
            <div style="text-align: center;">
                <iframe width="800" height="450" src="https://www.youtube.com/embed/qhRl-X95-y4?si=BNjNf8wWtkO17T4W"
                    title="YouTube video player" frameborder="0"
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                    referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                <p style="text-align: center; font-style: italic; color: grey; font-size: smaller;">YouTube video of the real-time effect recording of the 2022 Chinese New Year showcase.</p>
            </div>
            <h3>MR Multi-Player Shooting Game</h3>
            <p>
                This project is a mixed reality multi-player shooting game with rich property features.
            </p>
            <div style="text-align: center;">
                <iframe width="800" height="450" src="https://www.youtube.com/embed/BySqcemzbB0?si=lx_LLTzg7NM_NwJG"
                    title="YouTube video player" frameborder="0"
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                    referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                <p style="text-align: center; font-style: italic; color: grey; font-size: smaller;">YouTube video of the MR Multi-Player Shooting Game.</p>
            </div>
        </section>



        <!-- Contact Section with Social Media Links -->
        <section id="contact">

            <div class="social-media">
                <a href="https://www.linkedin.com/in/rongxuanmu/" target="_blank"><i class="fab fa-linkedin"></i></a>
                <a href="https://github.com/Rongxuan2024" target="_blank"><i class="fab fa-github"></i></a>
                <a href="https://space.bilibili.com/391939640" target="_blank"><i class="fab fa-bilibili"></i></a>
                <a href="https://www.youtube.com/@Xrchitect_Rong" target="_blank"><i class="fab fa-youtube"></i></a>
                <a href="https://murongxuan.itch.io/" target="_blank"><i class="fab fa-itch-io"></i></a>
                <a href="mailto:murongxuan1998@qq.com"><i class="fas fa-envelope"></i></a>
            </div>
        </section>
    </main>
    <footer>
        <p>&copy; 2024 Rongxuan. All rights reserved.</p>
    </footer>

</body>

</html>